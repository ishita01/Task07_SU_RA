# ğŸ§  Task 05 â€“ Descriptive Statistics with Large Language Models

## ğŸ“ Dataset
**Title**: Cars Datasets 2025  
**Source**: [Kaggle](https://www.kaggle.com/datasets/abdulmalik1518/cars-datasets-2025)  
**Size**: ~1,200 rows Ã— 11 columns  
**Content**: Details about cars such as company, model, engine size, horsepower, acceleration, top speed, price, fuel type, torque, and seating capacity.  
âš ï¸ *As instructed, the dataset file is excluded from this repository and is stored locally under `data/`.*

## ğŸ¯ Objective
The purpose of this task was to:  
- Select a compact, public dataset.  
- Produce descriptive and analytical statistics using Python.  
- Leverage a large language model (LLM) like ChatGPT to answer natural language queries about the dataset.  
- Verify the modelâ€™s outputs against computed values.  
- Reflect on prompt design, reasoning quality, and response accuracy.  

## ğŸ› ï¸ What I Did

### âœ… Step 1: Dataset Selection
I chose the Cars Datasets 2025 because it is clean, well-structured, and includes a wide range of attributes that support both numerical and reasoning-focused questions.  
ğŸ”— [Dataset link](https://www.kaggle.com/datasets/abdulmalik1518/cars-datasets-2025?resource=download)  

### âœ… Step 2: Descriptive Statistics
I developed a Python script (`basic_stats.py`) that:  
- Parsed numeric values from formatted text.  
- Calculated key summary measures: average horsepower, acceleration, engine size, torque, and price.  
- Ranked the top 5 cars by horsepower, acceleration, and price.  
- Generated grouped summaries: horsepower and price by fuel type, horsepower by seating capacity, acceleration by fuel type.  
- Computed derived ratios including horsepower-to-price, speed-to-acceleration, and speed-to-price.  

ğŸ“‚ Outputs saved to:  
- `output/summary_stats.json`  
- `output/top_performers.txt`  

### âœ… Step 3: Prompting the LLM
Using ChatGPT-4o, I created a set of natural language questions about the dataset, such as:  
- *"Which car has the highest horsepower?"*  
- *"Which fuel type accelerates the fastest on average?"*  
- *"If my budget is under $100,000, which high-performance cars should I consider?"*  

I documented:  
- Each prompt used  
- The LLMâ€™s answer  
- Whether the answer was fully correct, partially correct, or incorrect  
- Notes on refining prompts and observations of model behavior  

ğŸ‘‰ Full details can be found in `llm_prompt_log.md`.  

## ğŸ“‚ File Structure
```
Task_5/
â”œâ”€â”€ .gitignore                # Ignore sensitive files and dataset
â”œâ”€â”€ data/                     # Dataset (not uploaded)
â”‚   â””â”€â”€ Cars Datasets 2025.csv
â”œâ”€â”€ output/                   # Auto-generated statistics
â”‚   â”œâ”€â”€ summary_stats.json
â”‚   â””â”€â”€ top_performers.txt
â”œâ”€â”€ basic_stats.py            # Python script for statistics
â”œâ”€â”€ llm_prompt_log.md         # Prompt + response + validation
â””â”€â”€ README.md                 # Project documentation
```

## ğŸ§ª Tools Used
- Python 3.13  
- pandas, re, json, os  
- ChatGPT (GPT-4o, July 2025)  
- VS Code  

## ğŸ§  Key Learnings
- LLMs such as ChatGPT can process structured data effectively and give correct factual responses when prompts clearly reference column names.  
- For strategic or recommendation-style queries (e.g., best car under a price constraint), the model requires explicit metrics and clear guidance.  
- The way prompts were phrased had a major impact on accuracyâ€”small changes often improved responses.  

## ğŸ‘©â€ğŸ’» Author

**Ishita Ajay Trivedi**  
Masterâ€™s in Information Systems  
Syracuse University